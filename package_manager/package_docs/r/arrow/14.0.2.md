# R arrow Package v14.0.2 -- Approved Documentation

**Package:** arrow
**Version:** 14.0.2 (approved)
**Language:** R
**CRAN:** https://cran.r-project.org/package=arrow
**Validation Status:** Approved (2025-12-01)

---

## Overview

The `arrow` R package provides bindings to the Apache Arrow C++ library, enabling fast read/write of columnar data formats including Parquet and Arrow IPC. This is the primary output format for SDTM datasets in the orchestrator pipeline.

## Installation (Enterprise)

```r
# Use internal CRAN mirror only
install.packages('arrow', repos='https://internal-cran.company.com')

# Verify version
packageVersion('arrow')
# Expected: '14.0.2'
```

## Key Functions

### `read_parquet(file, ...)`
Read a Parquet file into an R data frame.

```r
# Basic usage
dm <- arrow::read_parquet("orchestrator/outputs/datasets/dm.parquet")

# Read specific columns only
dm_subset <- arrow::read_parquet("dm.parquet", col_select = c("USUBJID", "SEX", "AGE"))

# With Arrow Table (for large files)
table <- arrow::read_parquet("dm.parquet", as_data_frame = FALSE)
```

**Parameters:**
- `file` (character): Path to the Parquet file
- `col_select` (character vector, optional): Columns to read
- `as_data_frame` (logical, default TRUE): Return data.frame or Arrow Table
- `...`: Additional arguments passed to ParquetFileReader

### `write_parquet(x, sink, ...)`
Write a data frame to Parquet format.

```r
# Standard usage for SDTM output
arrow::write_parquet(dm, "orchestrator/outputs/datasets/dm.parquet")

# With explicit compression (default is snappy)
arrow::write_parquet(dm, "dm.parquet", compression = "snappy")
```

**Parameters:**
- `x` (data.frame or Arrow Table): Data to write
- `sink` (character): Output file path
- `compression` (character, default "snappy"): Compression algorithm
- `version` (character, default "2.6"): Parquet format version

**RESTRICTION:** Always use default compression (snappy). Do not change compression algorithm for validated datasets.

### `read_csv_arrow(file, ...)`
Fast CSV reader using the Arrow engine.

```r
# Faster alternative to read.csv for large files
raw_dm <- arrow::read_csv_arrow("study_data/raw_dm.csv")

# With explicit column types
raw_dm <- arrow::read_csv_arrow("raw_dm.csv",
  col_types = schema(SUBJID = utf8(), AGE = int32()))
```

**Parameters:**
- `file` (character): Path to CSV file
- `col_names` (logical or character vector): Column name handling
- `col_types` (Schema, optional): Explicit column types
- `skip` (integer): Rows to skip

### `open_dataset(sources, ...)`
Open a multi-file dataset for lazy evaluation.

```r
# Open a directory of Parquet files
ds <- arrow::open_dataset("data_dir/")
# Query with dplyr
result <- ds %>% filter(SEX == "M") %>% collect()
```

**RESTRICTION:** Do NOT use `open_dataset()` for streaming in validated programs. Use `read_parquet()` for explicit, deterministic loading.

## Version-Specific Notes (14.0.2)

- Fixed timestamp timezone handling on Windows (issue in 14.0.0)
- Large string columns (>2GB) supported since 14.0.0
- Default Parquet format version is 2.6
- Compatible with pyarrow 14.0.2 (Python) for cross-language Parquet files

## Common Patterns for SDTM

```r
# Read raw data, process, write SDTM output
library(arrow)

# Read
raw_dm <- read_csv_arrow("study_data/raw_dm.csv")

# ... (processing) ...

# Write primary output (Parquet)
write_parquet(dm, "orchestrator/outputs/datasets/dm.parquet")
```
